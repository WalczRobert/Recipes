{"cells":[{"cell_type":"code","source":["from __future__ import absolute_import, division, print_function\nimport gensim\nimport tensorflow as tf\nimport pandas as pd\nimport sqlalchemy\n\nprint('gensim version: \\t%s' % gensim.__version__)\nprint('TensorFlow version: \\t%s' % tf.__version__)\nprint('Pandas version: \\t%s' % pd.__version__)\nprint('SQLalchemy version: \\t%s' % sqlalchemy.__version__)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["df = spark.sql(\"select JD_SOLUTIONHTML from edl_current.product_casemanagement_dtacccms_product_casemanagement_dtac_qwsolutionslnhtml AS SOLUTION_DETAILS_FLAT inner join edl_current.product_casemanagement_dtacccms_product_casemanagement_dtac_qwsolution AS PCDTAC_SOLUTIONS ON (SOLUTION_DETAILS_FLAT.QW_SOLUTIONID = PCDTAC_SOLUTIONS.QW_SOLUTIONID) where PCDTAC_SOLUTIONS.DEERE_LANGUAGE_ID = '19' \")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["data_file_name = \"df\"\nraw_df = df.toPandas() #convert spark.sql to a pandas data frame\nprint(\"Data loaded\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["\n#raw_corpus = raw_df['JD_SOLUTIONHTML']\nraw_corpus = raw_corpus.join((str(raw_df['JD_SOLUTIONHTML']+\" \")))\nprint(\"Raw Corpus contains {0:,} characters\".format(len(raw_corpus)))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["import nltk\n# download the punkt tokenizer\nnltk.download('punkt')\nprint(\"The punkt tokenizer is downloaded\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\nprint(\"The punkt tokenizer is loaded\")\n# we tokenize the raw string into raw sentences\nraw_sentences = tokenizer.tokenize(raw_corpus)\nprint(\"We have {0:,} raw sentences\".format(len(raw_sentences)))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["import re\ndef clean_and_split_str(string):\n    strip_special_chars = re.compile(\"[^A-Za-z]+\")\n    string = re.sub(strip_special_chars, \" \", string)\n    return string.strip().split()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["sentences = []\nfor raw_sent in raw_sentences:\n    if len(raw_sent) > 0:\n        sentences.append(clean_and_split_str(raw_sent))\nprint(\"We have {0:,} clean sentences\".format(len(sentences)))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["print(raw_sentences[5])\nprint()\nprint(sentences[5])"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["token_count = sum([len(sentence) for sentence in sentences])\nprint(\"The dataset corpus contains {0:,} tokens\".format(token_count))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["import multiprocessing\n\n#Dimensionality of the resulting word vectors\nnum_features = 300\n\n#Minimum word count threshold\nmin_word_count = 3\n\n#Number of threads to run in parallel\nnum_workers = multiprocessing.cpu_count()\n\n#Context window length\ncontext_size = 7\n\n#Seed for the RNG, to make the result reproducible\nseed = 3"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["import gensim\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer\nword2vec_model = gensim.models.word2vec.Word2Vec(sg=1,\n    seed=seed,\n    workers=num_workers, \n    size=num_features, \n    min_count=min_word_count, \n    window=context_size)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["word2vec_model.build_vocab(sentences=sentences)\n\nprint(\"The vocabulary is built\")\nprint(\"Word2Vec vocabulary length: \", len(word2vec_model.wv.vocab))\nprint(word2vec_model.wv.vocab)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["#Start training the model\n\nword2vec_model.train(sentences, total_examples = token_count, epochs = 50)\nprint(\"Training finished\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["word2vec_model.wv.most_similar(positive=[\"SPEED\"])"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["if not os.path.exists(MODEL_DIR):\n    os.makedirs(MODEL_DIR)\nmodel.save(os.path.join(MODEL_DIR,'word2vec'))"],"metadata":{},"outputs":[],"execution_count":19}],"metadata":{"name":"Gensim_Word2Vec","notebookId":13287597},"nbformat":4,"nbformat_minor":0}
